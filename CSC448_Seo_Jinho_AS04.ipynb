{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PineappleBingo/CSC448-AI/blob/main/CSC448_Seo_Jinho_AS04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S8PKiVJaL_AW"
      },
      "outputs": [],
      "source": [
        "# Imports and pip installations (if needed)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A05Q5B0_NUX9"
      },
      "source": [
        "# Part 1: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "YZ4MUsbXNZ0P",
        "outputId": "77e7a628-792f-4d28-b7b1-e06f4db3efd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                 5.1               3.5                1.4               0.2   \n",
            "1                 4.9               3.0                1.4               0.2   \n",
            "2                 4.7               3.2                1.3               0.2   \n",
            "3                 4.6               3.1                1.5               0.2   \n",
            "4                 5.0               3.6                1.4               0.2   \n",
            "5                 5.4               3.9                1.7               0.4   \n",
            "6                 4.6               3.4                1.4               0.3   \n",
            "7                 5.0               3.4                1.5               0.2   \n",
            "8                 4.4               2.9                1.4               0.2   \n",
            "9                 4.9               3.1                1.5               0.1   \n",
            "10                5.4               3.7                1.5               0.2   \n",
            "11                4.8               3.4                1.6               0.2   \n",
            "12                4.8               3.0                1.4               0.1   \n",
            "13                4.3               3.0                1.1               0.1   \n",
            "14                5.8               4.0                1.2               0.2   \n",
            "\n",
            "    target  \n",
            "0        0  \n",
            "1        0  \n",
            "2        0  \n",
            "3        0  \n",
            "4        0  \n",
            "5        0  \n",
            "6        0  \n",
            "7        0  \n",
            "8        0  \n",
            "9        0  \n",
            "10       0  \n",
            "11       0  \n",
            "12       0  \n",
            "13       0  \n",
            "14       0  \n"
          ]
        }
      ],
      "source": [
        "# Load the dataset (load remotely, not locally)\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "# columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n",
        "\n",
        "df[\"target\"] = iris.target\n",
        "\n",
        "# Output the first 15 rows of the data\n",
        "print(df.head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vMX1_Wdm3Agn",
        "outputId": "64ffe3d3-cc2a-4b3f-bbd3-be3349b4a3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   sepal length (cm)  150 non-null    float64\n",
            " 1   sepal width (cm)   150 non-null    float64\n",
            " 2   petal length (cm)  150 non-null    float64\n",
            " 3   petal width (cm)   150 non-null    float64\n",
            " 4   target             150 non-null    int32  \n",
            "dtypes: float64(4), int32(1)\n",
            "memory usage: 5.4 KB\n",
            "None\n",
            "\n",
            "Row x Columns: (150, 5)\n",
            "\n",
            "Number of dataset: 150\n"
          ]
        }
      ],
      "source": [
        "# Display a summary of the table information (number of datapoints, etc.)\n",
        "print(df.info())\n",
        "print(\"\\nRow x Columns:\", df.shape)\n",
        "print(\"\\nNumber of dataset:\", df.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRMtsJKBaxWu"
      },
      "source": [
        "## About the dataset\n",
        "Explain what the data is in your own words. What are your features and labels? What is the mapping of your labels to the actual classes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c-gk6nzq3Ago",
        "outputId": "3abcc1bb-ee48-4be4-dd32-027f32994875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iris features:\n",
            "sepal length (cm)\n",
            "sepal width (cm)\n",
            "petal length (cm)\n",
            "petal width (cm)\n",
            "\n",
            "Label: Iris Species\n",
            "\n",
            "The Mapping:\n",
            "0 Setosa\n",
            "1 Versicolor\n",
            "2 Virginica\n"
          ]
        }
      ],
      "source": [
        "# The Iris Dataset is a multi-class classification flower dataset that \n",
        "# contains 150 records from three different types of species: \n",
        "# Iris setosa, Iris versicolor and Iris virginica. \n",
        "\n",
        "# Features:\n",
        "IrisF = df.columns\n",
        "print(\"Iris features:\")\n",
        "for i in range(0,4):\n",
        "    print(IrisF[i])\n",
        "\n",
        "# Label:\n",
        "# Iris species\n",
        "print(\"\\nLabel: Iris Species\")\n",
        "\n",
        "# The mapping:\n",
        "print(\"\\nThe Mapping:\")\n",
        "Y = iris.target\n",
        "IC = np.unique(Y)\n",
        "IrisC = ('Setosa','Versicolor','Virginica')\n",
        "\n",
        "for i in range(0,3):\n",
        "    print (IC[i], IrisC[i])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhKaIrZKNaHg"
      },
      "source": [
        "# Part 2: Split the dataset into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OrgogB62NcOi"
      },
      "outputs": [],
      "source": [
        "# Take the dataset and split it into our features (X) and label (y)\n",
        "\n",
        "# features X\n",
        "features = df[iris.feature_names]\n",
        "# lable Y\n",
        "label = df[\"target\"]\n",
        "\n",
        "X, Y = features, label \n",
        "\n",
        "# Use sklearn to split the features and labels into a training/test set. (90% train, 10% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblF-ei9Ncia"
      },
      "source": [
        "# Part 3: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhFhEN03Nf7o",
        "outputId": "15a94661-020b-4520-b5f3-c34fdfb042f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability for each possible classes: \n",
            "[2 3 2 3] Setosa     -> probability:0.9847075998816394\n",
            "[2 3 2 3] Versicolor -> probability:0.013102699516003286\n",
            "[2 3 2 3] Virginica  -> probability:0.0021897006023574236\n",
            "------------------------------------------------------\n",
            "The score for the Logistic Regression Model:  1.0\n",
            "------------------------------------------------------\n",
            "Coefficients for each possible classes:\n",
            "[2 3 2 3] Setosa     -> Coefficients:[-0.42179578  0.93564604 -2.43711991 -1.05535041]\n",
            "[2 3 2 3] Versicolor -> Coefficients:[ 0.53578775 -0.25471659 -0.2074305  -0.8459005 ]\n",
            "[2 3 2 3] Virginica  -> Coefficients:[-0.11399197 -0.68092946  2.64455041  1.90125091]\n",
            "------------------------------------------------------\n",
            "Intercepts for each possible classe:\n",
            "[2 3 2 3] Setosa     -> Intercept: 9.552979926317207\n",
            "[2 3 2 3] Versicolor -> Intercept: 1.8016457728697521\n",
            "[2 3 2 3] Virginica  -> Intercept: -11.354625699187194\n",
            "------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# i. Use sklearn to train a LogisticRegression model on the training set\n",
        "log_reg_model  = LogisticRegression().fit(X_train, Y_train)\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "sample = np.array([[2,3,2,3]])\n",
        "\n",
        "log_model_prob = log_reg_model.predict_proba(sample)\n",
        "\n",
        "print(\"Probability for each possible classes: \")\n",
        "for idx in range(0, 3):\n",
        "    print(\"{:<10}{:<10}{}{}\".format(str(sample[0]), str(IrisC[idx]),\" -> probability:\", log_model_prob[0][idx]))\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "# iii. Report on the score for Logistic regression model, what does the score measure?\n",
        "log_model_score = log_reg_model.score(X_test, Y_test)\n",
        "print(\"The score for the Logistic Regression Model: \", log_model_score)\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "# It measures the performance or accuracy of the Logistic Regression model.\n",
        "# The score 100% means that this model correctly classifying each classes of dataset\n",
        "\n",
        "\n",
        "# iv. Extract the coefficents and Intercepts for the boundary line(s)\n",
        "coefficients = log_reg_model.coef_\n",
        "print(\"Coefficients for each possible classes:\")\n",
        "for idx in range(0, 3):\n",
        "    print(\"{:<10}{:<10}{}{}\".format(str(sample[0]), str(IrisC[idx]), \" -> Coefficients:\", coefficients[idx]))\n",
        "\n",
        "print(\"------------------------------------------------------\")\n",
        "# intercept \n",
        "intercept  = log_reg_model.intercept_\n",
        "print(\"Intercepts for each possible classe:\")\n",
        "for idx in range(0, 3):\n",
        "    print(\"{:<10}{:<10}{}{}\".format(str(sample[0]), str(IrisC[idx]), \" -> Intercept: \", intercept[idx]))\n",
        "print(\"------------------------------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDUpXQN4Nilk"
      },
      "source": [
        "# Part 4: Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U__zukpdNqiQ",
        "outputId": "5cd81a50-9137-4f36-842e-0687e9bc7c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability for each possible class:  [0]\n",
            "------------------------------------------------------\n",
            "Accuracy of the SVC Clasification is:  1.0\n",
            "------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# i. Use sklearn to train a Support Vector Classifier on the training set\n",
        "\n",
        "svm_model = SVC(probability=True)\n",
        "svm_model.fit(X_train, Y_train)\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "\n",
        "# predict_proba will give the possibility of each class given the sample point\n",
        "# The model predicts that each of the probabilities belongs to each class [0,1,2]\n",
        "# classes = np.array([[2,3,2,3]])\n",
        "svm_pred = svm_model.predict(np.array(sample))\n",
        "\n",
        "print(\"Probability for each possible class: \", svm_pred)\n",
        "# [0]: Setosa Probability\n",
        "\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "# iii. Report on the score for the SVM, what does the score measure?\n",
        "svm_model_score = svm_model.score(X_test, Y_test)\n",
        "print(\"Accuracy of the SVC Clasification is: \", svm_model_score)\n",
        "\n",
        "# it measures that the classification is %100 correct\n",
        "print(\"------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULoL7mMBNrlS"
      },
      "source": [
        "# Part 5: Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKKmODVrN9lQ",
        "outputId": "dced24f7-c6bd-45fb-c68f-87548e9228dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability for each possible classes: \n",
            "[2 3 2 3] Setosa     -> probability: 3.694674403478e-239\n",
            "[2 3 2 3] Versicolor -> probability: 1.0\n",
            "[2 3 2 3] Virginica  -> probability: 4.1277849627003388e-22\n",
            "------------------------------------------------------\n",
            "Accuracy of the Neural Network Clasification:  1.0\n",
            "------------------------------------------------------\n",
            "Probability for each possible classes: \n",
            "[2 3 2 3] Setosa     -> probability: 0.6182893047418228\n",
            "[2 3 2 3] Versicolor -> probability: 0.1344155881768067\n",
            "[2 3 2 3] Virginica  -> probability: 0.2472951070813706\n",
            "------------------------------------------------------\n",
            "Accuracy of the Neural Network Clasification:  0.5333333333333333\n",
            "------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "D:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "D:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# i. Use sklearn to train a Neural Network (MLP Classifier) on the training set\n",
        "\n",
        "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(15,),random_state=1).fit(features, label)\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "# sample = np.array([[2,3,2,3]])\n",
        "mlp_model_pred = mlp_model.predict_proba(sample)\n",
        "\n",
        "print(\"Probability for each possible classes: \")\n",
        "for idx in range(0, 3):\n",
        "    print(\"{:<10}{:<10}{}{}\".format(str(sample[0]), str(IrisC[idx]),\" -> probability: \", mlp_model_pred[0][idx]))\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "# iii. Report on the score for the Neural Network, what does the score measure?\n",
        "mlp_model_score = mlp_model.score(X_test, Y_test)\n",
        "print(\"Accuracy of the Neural Network Clasification: \", mlp_model_score)\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "# iv: Experiment with different options for the neural network, report on your best configuration (the highest score I was able to achieve was 0.8666)\n",
        "mlp_model = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=(15,),random_state=1).fit(features, label)\n",
        "\n",
        "# sample = np.array([[2,3,2,3]])\n",
        "mlp_model_pred = mlp_model.predict_proba(sample)\n",
        "\n",
        "print(\"Probability for each possible classes: \")\n",
        "for idx in range(0, 3):\n",
        "    print(\"{:<10}{:<10}{}{}\".format(str(sample[0]), str(IrisC[idx]),\" -> probability: \", mlp_model_pred[0][idx]))\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "mlp_model_score = mlp_model.score(X_test, Y_test)\n",
        "print(\"Accuracy of the Neural Network Clasification: \", mlp_model_score)\n",
        "print(\"------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bi5tDwHmC28"
      },
      "source": [
        "# Part 6: K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCFlfJy2mCg6",
        "outputId": "e71bf88c-6418-4b7f-e289-4acf743c16cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability for each possible classes: \n",
            "[2 3 2 3] Setosa     -> probability: 0.7777777777777778\n",
            "[2 3 2 3] Versicolor -> probability: 0.2222222222222222\n",
            "[2 3 2 3] Virginica  -> probability: 0.0\n",
            "------------------------------------------------------\n",
            "Accuracy of the K-Nearest Neighbors Clasification:  0.9333333333333333\n",
            "------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# i. Use sklearn to 'train' a k-Neighbors Classifier\n",
        "# Note: KNN is a nonparametric model and technically doesn't require training\n",
        "# fit will essentially load the data into the model see link below for more information\n",
        "# https://stats.stackexchange.com/questions/349842/why-do-we-need-to-fit-a-k-nearest-neighbors-classifier\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=9)\n",
        "knn_model.fit(X_train, Y_train)\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "# sample = np.array([[2,3,2,3]])\n",
        "knn_model_pred = knn_model.predict_proba(sample)\n",
        "print(\"Probability for each possible classes: \")\n",
        "for idx in range(0, 3):\n",
        "    print(\"{:<10}{:<10}{}{}\".format(str(sample[0]), str(IrisC[idx]),\" -> probability: \", knn_model_pred[0][idx]))\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "\n",
        "# iii. Report on the score for kNN, what does the score measure?\n",
        "knn_model_score = knn_model.score(X_test, Y_test)\n",
        "print(\"Accuracy of the K-Nearest Neighbors Clasification: \", knn_model_score)\n",
        "print(\"------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "162Sw3LeoqE2"
      },
      "source": [
        "# Part 7: Conclusions and takeaways\n",
        "\n",
        "In your own words describe the results of the notebook. Which model(s) performed the best on the dataset? Why do you think that is? Did anything surprise you about the exercise?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_TEgjvFH3Agr"
      },
      "outputs": [],
      "source": [
        "# According to the result of each models' accuracy scores, the performance score was very similar for all the predition models. The most accurated models are \n",
        "# logistic regression model and SVM. Most cases each model classified the sample dataset predicted as [0]:Setosa.  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "CSC448_Seo_Jinho_AS04.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
